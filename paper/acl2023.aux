\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{brown2020gpt3}
\citation{kaplan2020scaling}
\citation{chowdhery2022palm,rae2021goepher}
\citation{hoffmann2022chinchilla}
\citation{hoffmann2022chinchilla}
\citation{hoffmann2022chinchilla}
\citation{zhang2022opt}
\citation{black2022gpt}
\citation{scao2022bloom}
\citation{zhengGLM2022}
\citation{vaswaniAttention2017}
\citation{brown2020gpt3,chowdhery2022palm}
\citation{hoffmann2022chinchilla}
\citation{wenzek-etal-2020-ccnet}
\citation{raffel2020exploring}
\citation{pile}
\citation{lewkowycz2022solving}
\citation{sennrich2015neural}
\citation{kudo2018sentencepiece}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:dataset}{{1}{2}{\textbf {Pre-training data.} Data mixtures used for pre-training, for each subset we list the sampling proportion, number of epochs performed on the subset when training on 1.4T tokens, and disk size. The pre-training runs on 1T tokens have the same sampling proportion.  \relax }{table.caption.5}{}}
\citation{vaswaniAttention2017}
\citation{zhang2019root}
\citation{shazeer2020glu}
\citation{su2021roformer}
\citation{loshchilov2017decoupled}
\citation{rabe2021self}
\citation{dao2022flashattention}
\citation{korthikanti2022reducing}
\newlabel{tab:architecture}{{2}{3}{\textbf {Model sizes, architectures, and optimization hyper-parameters.}  \relax }{table.caption.9}{}}
\newlabel{fig:trainincurves}{{1}{3}{\textbf {Training loss over train tokens for the 7B, 13B, 33B, and 65 models.} \model -33B and \model -65B were trained on 1.4T tokens. The smaller models were trained on 1.0T tokens. All models are trained with a batch size of 4M tokens.  \relax }{figure.caption.14}{}}
\citation{brown2020gpt3}
\citation{brown2020gpt3}
\citation{rae2021goepher}
\citation{hoffmann2022chinchilla}
\citation{chowdhery2022palm}
\citation{zhang2022opt}
\citation{gptj}
\citation{black2022gpt}
\citation{iyer2022opt}
\citation{Chung2022ScalingIL}
\citation{eval-harness}
\citation{brown2020gpt3}
\citation{clark2019boolq}
\citation{bisk2020piqa}
\citation{sap2019socialiqa}
\citation{zellers2019hellaswag}
\citation{sakaguchi2021winogrande}
\citation{clark2018think}
\citation{mihaylov2018can}
\newlabel{tab:commonsense}{{3}{4}{\textbf {Zero-shot performance on Common Sense Reasoning tasks.}  \relax }{table.caption.15}{}}
\newlabel{tab:nqa}{{4}{4}{\textbf {NaturalQuestions.} Exact match performance.  \relax }{table.caption.16}{}}
\citation{kwiatkowski2019natural}
\citation{joshi2017triviaqa}
\citation{lai2017race}
\citation{brown2020gpt3}
\citation{hendrycks2021measuring}
\citation{cobbe2021training}
\citation{lewkowycz2022solving}
\citation{lewkowycz2022solving}
\citation{wang2022Self}
\citation{chen2021Evaluating}
\citation{austin2021program}
\citation{thoppilan2022lambda}
\newlabel{tab:tqa}{{5}{5}{\textbf {TriviaQA.} Zero-shot and few-shot exact match performance on the filtered dev set.  \relax }{table.caption.17}{}}
\newlabel{tab:readingcomprehension}{{6}{5}{\textbf {Reading Comprehension.} Zero-shot accuracy.  \relax }{table.caption.18}{}}
\newlabel{sec:codegen}{{3.5}{5}{Code generation}{subsection.3.5}{}}
\citation{chen2021Evaluating}
\citation{chowdhery2022palm}
\citation{chen2021Evaluating,nijkamp2022codegen,fried2022incoder}
\citation{austin2021program}
\citation{chowdhery2022palm}
\citation{austin2021program}
\citation{chowdhery2022palm}
\citation{hendrycks2020measuring}
\newlabel{tab:math}{{7}{6}{\textbf {Model performance on quantitative reasoning datasets.} For majority voting, we use the same setup as Minerva, with $k=256$ samples for MATH and $k=100$ for GSM8k (Minerva 540B uses $k=64$ for MATH and and $k=40$ for GSM8k). \model -65B outperforms Minerva 62B on GSM8k, although it has not been fine-tuned on mathematical data.  \relax }{table.caption.19}{}}
\newlabel{tab:code}{{8}{6}{\textbf {Model performance for code generation.} We report the pass@ score on HumanEval and MBPP. HumanEval generations are done in zero-shot and MBBP with 3-shot prompts similar to~\citet {austin2021program}. The values marked with $^*$ are read from figures in~\citet {chowdhery2022palm}.  \relax }{table.caption.20}{}}
\citation{Chung2022ScalingIL}
\citation{iyer2022opt}
\citation{Chung2022ScalingIL}
\citation{iyer2022opt}
\citation{sheng2019woman,kurita2019quantifying}
\citation{gehman2020realtoxicityprompts}
\citation{zhang2022opt,hoffmann2022chinchilla}
\citation{gehman2020realtoxicityprompts}
\newlabel{tab:mmlu}{{9}{7}{\textbf {Massive Multitask Language Understanding (MMLU).} Five-shot accuracy.  \relax }{table.caption.21}{}}
\newlabel{sec:instruct}{{4}{7}{Instruction Finetuning}{section.4}{}}
\newlabel{tab:instruct}{{10}{7}{\textbf {Instruction finetuning -- MMLU (5-shot).} Comparison of models of moderate size with and without instruction finetuning on MMLU.  \relax }{table.caption.23}{}}
\citation{zhang2022opt}
\citation{hoffmann2022chinchilla}
\citation{nangia2020crows}
\newlabel{fig:evals}{{2}{8}{\textbf {Evolution of performance on question answering and common sense reasoning during training.}  \relax }{figure.caption.22}{}}
\newlabel{tab:RealToxicityPrompts}{{11}{8}{\textbf {RealToxicityPrompts.} We run a greedy decoder on the 100k prompts from this benchmark. The ``respectful'' versions are prompts starting with ``Complete the following sentence in a polite, respectful, and unbiased manner:'', and ``Basic'' is without it. Scores were obtained using the PerplexityAPI, with higher score indicating more toxic generations.  \relax }{table.caption.24}{}}
\citation{rudinger2018gender}
\citation{rae2021goepher,hoffmann2022chinchilla}
\citation{lin2021truthfulqa}
\citation{lin2021truthfulqa}
\newlabel{tab:crows}{{12}{9}{\textbf {CrowS-Pairs.} We compare the level of biases contained in \model -65B with OPT-175B and GPT3-175B. Higher score indicates higher bias. \relax }{table.caption.25}{}}
\citation{ouyang2022training}
\citation{ouyang2022training}
\citation{wu2022sustainable}
\citation{wu2022sustainable}
\citation{wu2022sustainable}
\citation{shannon1948mathematical,shannon1951prediction}
\citation{bahl1983maximum,brown1990statistical}
\citation{turing1950computing}
\citation{mahoney1999text}
\citation{bahl1983maximum}
\citation{katz1987estimation,kneser1995improved}
\citation{bengio2000neural}
\citation{elman1990finding,mikolov2010recurrent}
\citation{hochreiter1997long,graves2013generating}
\citation{vaswaniAttention2017,radford2018improving,dai2019transformer}
\newlabel{tab:winogender}{{13}{10}{\textbf {WinoGender.} Co-reference resolution accuracy for the \model models, for different pronouns (``her/her/she'' and ``his/him/he''). We observe that our models obtain better performance on ``their/them/someone' pronouns than on ``her/her/she'' and ``his/him/he', which is likely indicative of biases.\relax }{table.caption.26}{}}
\newlabel{tab:TuthfulQA}{{14}{10}{\textbf {TruthfulQA.} We report the fraction of truthful and truthful*informative answers, as scored by specially trained models via the OpenAI API. We follow the QA prompt style used in \citet {ouyang2022training}, and report the performance of GPT-3 from the same paper. \relax }{table.caption.27}{}}
\citation{brants2007large}
\citation{heafield2013scalable}
\citation{buck2014n}
\citation{chelba2013one}
\citation{jozefowicz2016exploring}
\citation{devlin2018bert}
\citation{radford2019language}
\citation{shoeybi2019megatron}
\citation{raffel2020exploring}
\citation{brown2020gpt3}
\citation{lieber2021jurassic}
\citation{smith2022using}
\citation{rae2021goepher}
\citation{hoffmann2022chinchilla}
\citation{chowdhery2022palm}
\citation{zhang2022opt}
\citation{zhengGLM2022}
\citation{hestness2017deep}
\citation{rosenfeld2019constructive}
\citation{kaplan2020scaling}
\citation{hoffmann2022chinchilla}
\citation{wei2022emergent}
\citation{Chung2022ScalingIL}
\newlabel{tab:cf}{{15}{11}{\textbf {Carbon footprint of training different models in the same data center.} We follow~\citet {wu2022sustainable} to compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power consumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a PUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO$_2$e per KWh.  \vspace {-0.5em} \relax }{table.caption.28}{}}
\bibdata{custom}
\bibcite{austin2021program}{{1}{2021}{{Austin et~al.}}{{Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton}}}
\bibcite{bahl1983maximum}{{2}{1983}{{Bahl et~al.}}{{Bahl, Jelinek, and Mercer}}}
\bibcite{bengio2000neural}{{3}{2000}{{Bengio et~al.}}{{Bengio, Ducharme, and Vincent}}}
\bibcite{bisk2020piqa}{{4}{2020}{{Bisk et~al.}}{{Bisk, Zellers, Gao, Choi et~al.}}}
\bibcite{black2022gpt}{{5}{2022}{{Black et~al.}}{{Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang et~al.}}}
\bibcite{brants2007large}{{6}{2007}{{Brants et~al.}}{{Brants, Popat, Xu, Och, and Dean}}}
\bibcite{brown1990statistical}{{7}{1990}{{Brown et~al.}}{{Brown, Cocke, Della~Pietra, Della~Pietra, Jelinek, Lafferty, Mercer, and Roossin}}}
\bibcite{brown2020gpt3}{{8}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}}}
\bibcite{buck2014n}{{9}{2014}{{Buck et~al.}}{{Buck, Heafield, and Van~Ooyen}}}
\bibcite{chelba2013one}{{10}{2013}{{Chelba et~al.}}{{Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and Robinson}}}
\bibcite{chen2021Evaluating}{{11}{2021}{{Chen et~al.}}{{Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba}}}
\bibcite{chowdhery2022palm}{{12}{2022}{{Chowdhery et~al.}}{{Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel}}}
\bibcite{Chung2022ScalingIL}{{13}{2022}{{Chung et~al.}}{{Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Valter, Narang, Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, hsin Chi, Dean, Devlin, Roberts, Zhou, Le, and Wei}}}
\bibcite{clark2019boolq}{{14}{2019}{{Clark et~al.}}{{Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}}}
\bibcite{clark2018think}{{15}{2018}{{Clark et~al.}}{{Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}}}
\bibcite{cobbe2021training}{{16}{2021}{{Cobbe et~al.}}{{Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}}}
\bibcite{dai2019transformer}{{17}{2019}{{Dai et~al.}}{{Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov}}}
\bibcite{dao2022flashattention}{{18}{2022}{{Dao et~al.}}{{Dao, Fu, Ermon, Rudra, and R{\'e}}}}
\bibcite{devlin2018bert}{{19}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{elman1990finding}{{20}{1990}{{Elman}}{{}}}
\bibcite{fried2022incoder}{{21}{2022}{{Fried et~al.}}{{Fried, Aghajanyan, Lin, Wang, Wallace, Shi, Zhong, Yih, Zettlemoyer, and Lewis}}}
\bibcite{pile}{{22}{2020}{{Gao et~al.}}{{Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy}}}
\bibcite{eval-harness}{{23}{2021}{{Gao et~al.}}{{Gao, Tow, Biderman, Black, DiPofi, Foster, Golding, Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and Zou}}}
\bibcite{gehman2020realtoxicityprompts}{{24}{2020}{{Gehman et~al.}}{{Gehman, Gururangan, Sap, Choi, and Smith}}}
\bibcite{graves2013generating}{{25}{2013}{{Graves}}{{}}}
\bibcite{heafield2013scalable}{{26}{2013}{{Heafield et~al.}}{{Heafield, Pouzyrevsky, Clark, and Koehn}}}
\bibcite{hendrycks2020measuring}{{27}{2020}{{Hendrycks et~al.}}{{Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}}}
\bibcite{hendrycks2021measuring}{{28}{2021}{{Hendrycks et~al.}}{{Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}}}
\bibcite{hestness2017deep}{{29}{2017}{{Hestness et~al.}}{{Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Ali, Yang, and Zhou}}}
\bibcite{hochreiter1997long}{{30}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hoffmann2022chinchilla}{{31}{2022}{{Hoffmann et~al.}}{{Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre}}}
\bibcite{iyer2022opt}{{32}{2022}{{Iyer et~al.}}{{Iyer, Lin, Pasunuru, Mihaylov, Simig, Yu, Shuster, Wang, Liu, Koura et~al.}}}
\bibcite{joshi2017triviaqa}{{33}{2017}{{Joshi et~al.}}{{Joshi, Choi, Weld, and Zettlemoyer}}}
\bibcite{jozefowicz2016exploring}{{34}{2016}{{Jozefowicz et~al.}}{{Jozefowicz, Vinyals, Schuster, Shazeer, and Wu}}}
\bibcite{kaplan2020scaling}{{35}{2020}{{Kaplan et~al.}}{{Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}}}
\bibcite{katz1987estimation}{{36}{1987}{{Katz}}{{}}}
\bibcite{kneser1995improved}{{37}{1995}{{Kneser and Ney}}{{}}}
\bibcite{korthikanti2022reducing}{{38}{2022}{{Korthikanti et~al.}}{{Korthikanti, Casper, Lym, McAfee, Andersch, Shoeybi, and Catanzaro}}}
\bibcite{kudo2018sentencepiece}{{39}{2018}{{Kudo and Richardson}}{{}}}
\bibcite{kurita2019quantifying}{{40}{2019}{{Kurita et~al.}}{{Kurita, Vyas, Pareek, Black, and Tsvetkov}}}
\bibcite{kwiatkowski2019natural}{{41}{2019}{{Kwiatkowski et~al.}}{{Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee et~al.}}}
\bibcite{lai2017race}{{42}{2017}{{Lai et~al.}}{{Lai, Xie, Liu, Yang, and Hovy}}}
\bibcite{lewkowycz2022solving}{{43}{2022}{{Lewkowycz et~al.}}{{Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, Wu, Neyshabur, Gur-Ari, and Misra}}}
\bibcite{lieber2021jurassic}{{44}{2021}{{Lieber et~al.}}{{Lieber, Sharir, Lenz, and Shoham}}}
\bibcite{lin2021truthfulqa}{{45}{2021}{{Lin et~al.}}{{Lin, Hilton, and Evans}}}
\bibcite{loshchilov2017decoupled}{{46}{2017}{{Loshchilov and Hutter}}{{}}}
\bibcite{mahoney1999text}{{47}{1999}{{Mahoney}}{{}}}
\bibcite{mihaylov2018can}{{48}{2018}{{Mihaylov et~al.}}{{Mihaylov, Clark, Khot, and Sabharwal}}}
\bibcite{mikolov2010recurrent}{{49}{2010}{{Mikolov et~al.}}{{Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and Khudanpur}}}
\bibcite{nangia2020crows}{{50}{2020}{{Nangia et~al.}}{{Nangia, Vania, Bhalerao, and Bowman}}}
\bibcite{nijkamp2022codegen}{{51}{2022}{{Nijkamp et~al.}}{{Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese, and Xiong}}}
\bibcite{ouyang2022training}{{52}{2022}{{Ouyang et~al.}}{{Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Gray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe}}}
\bibcite{rabe2021self}{{53}{2021}{{Rabe and Staats}}{{}}}
\bibcite{radford2018improving}{{54}{2018}{{Radford et~al.}}{{Radford, Narasimhan, Salimans, Sutskever et~al.}}}
\bibcite{radford2019language}{{55}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, Sutskever et~al.}}}
\bibcite{rae2021goepher}{{56}{2021}{{Rae et~al.}}{{Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving}}}
\bibcite{raffel2020exploring}{{57}{2020}{{Raffel et~al.}}{{Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}}}
\bibcite{rosenfeld2019constructive}{{58}{2019}{{Rosenfeld et~al.}}{{Rosenfeld, Rosenfeld, Belinkov, and Shavit}}}
\bibcite{rudinger2018gender}{{59}{2018}{{Rudinger et~al.}}{{Rudinger, Naradowsky, Leonard, and Van~Durme}}}
\bibcite{sakaguchi2021winogrande}{{60}{2021}{{Sakaguchi et~al.}}{{Sakaguchi, Bras, Bhagavatula, and Choi}}}
\bibcite{sap2019socialiqa}{{61}{2019}{{Sap et~al.}}{{Sap, Rashkin, Chen, LeBras, and Choi}}}
\bibcite{scao2022bloom}{{62}{2022}{{Scao et~al.}}{{Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e} et~al.}}}
\bibcite{sennrich2015neural}{{63}{2015}{{Sennrich et~al.}}{{Sennrich, Haddow, and Birch}}}
\bibcite{shannon1948mathematical}{{64}{1948}{{Shannon}}{{}}}
\bibcite{shannon1951prediction}{{65}{1951}{{Shannon}}{{}}}
\bibcite{shazeer2020glu}{{66}{2020}{{Shazeer}}{{}}}
\bibcite{sheng2019woman}{{67}{2019}{{Sheng et~al.}}{{Sheng, Chang, Natarajan, and Peng}}}
\bibcite{shoeybi2019megatron}{{68}{2019}{{Shoeybi et~al.}}{{Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro}}}
\bibcite{smith2022using}{{69}{2022}{{Smith et~al.}}{{Smith, Patwary, Norick, LeGresley, Rajbhandari, Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zhang, Child, Aminabadi, Bernauer, Song, Shoeybi, He, Houston, Tiwary, and Catanzaro}}}
\bibcite{su2021roformer}{{70}{2021}{{Su et~al.}}{{Su, Lu, Pan, Murtadha, Wen, and Liu}}}
\bibcite{thoppilan2022lambda}{{71}{2022}{{Thoppilan et~al.}}{{Thoppilan, De~Freitas, Hall, Shazeer, Kulshreshtha, Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali, Huang, Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhao, Zhou, Chang, Krivokon, Rusch, Pickett, Srinivasan, Man, Meier-Hellstern, Morris, Doshi, Santos, Duke, Soraker, Zevenbergen, Prabhakaran, Diaz, Hutchinson, Olson, Molina, Hoffman-John, Lee, Aroyo, Rajakumar, Butryna, Lamm, Kuzmina, Fenton, Cohen, Bernstein, Kurzweil, Aguera-Arcas, Cui, Croak, Chi, and Le}}}
\bibcite{turing1950computing}{{72}{1950}{{Turing}}{{}}}
\bibcite{vaswaniAttention2017}{{73}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{gptj}{{74}{2021}{{Wang and Komatsuzaki}}{{}}}
\bibcite{wang2022Self}{{75}{2022}{{Wang et~al.}}{{Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}}}
\bibcite{wei2022emergent}{{76}{2022}{{Wei et~al.}}{{Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler et~al.}}}
\bibcite{wenzek-etal-2020-ccnet}{{77}{2020}{{Wenzek et~al.}}{{Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave}}}
\bibcite{wu2022sustainable}{{78}{2022}{{Wu et~al.}}{{Wu, Raghavendra, Gupta, Acun, Ardalani, Maeng, Chang, Aga, Huang, Bai et~al.}}}
\bibcite{zellers2019hellaswag}{{79}{2019}{{Zellers et~al.}}{{Zellers, Holtzman, Bisk, Farhadi, and Choi}}}
\bibcite{zhengGLM2022}{{80}{2022}{{Zeng et~al.}}{{Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia, Tam, Ma, Xue, Zhai, Chen, Zhang, Dong, and Tang}}}
\bibcite{zhang2019root}{{81}{2019}{{Zhang and Sennrich}}{{}}}
\bibcite{zhang2022opt}{{82}{2022}{{Zhang et~al.}}{{Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}}}
\bibstyle{acl_natbib}
\newlabel{fig:qa}{{3}{I}{Formatted dataset example for Natural Questions (left) \& TriviaQA (right).\relax }{figure.caption.34}{}}
\newlabel{tab:mmluapp}{{16}{II}{\textbf {MMLU.} Detailed 5-shot results per domain on the test sets.  \relax }{table.caption.36}{}}
\citation{Chung2022ScalingIL}
\newlabel{sec:prompt}{{D}{VI}{Generations from \model -I}{appendix.D}{}}
\gdef \@abspage@last{27}
